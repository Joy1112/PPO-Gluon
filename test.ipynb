{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, autograd\n",
    "import mxnet.ndarray as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Conv2D in module mxnet.gluon.nn.conv_layers:\n",
      "\n",
      "class Conv2D(_Conv)\n",
      " |  2D convolution layer (e.g. spatial convolution over images).\n",
      " |  \n",
      " |  This layer creates a convolution kernel that is convolved\n",
      " |  with the layer input to produce a tensor of\n",
      " |  outputs. If `use_bias` is True,\n",
      " |  a bias vector is created and added to the outputs. Finally, if\n",
      " |  `activation` is not `None`, it is applied to the outputs as well.\n",
      " |  \n",
      " |  If `in_channels` is not specified, `Parameter` initialization will be\n",
      " |  deferred to the first time `forward` is called and `in_channels` will be\n",
      " |  inferred from the shape of input data.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  channels : int\n",
      " |      The dimensionality of the output space, i.e. the number of output\n",
      " |      channels (filters) in the convolution.\n",
      " |  kernel_size :int or tuple/list of 2 int\n",
      " |      Specifies the dimensions of the convolution window.\n",
      " |  strides : int or tuple/list of 2 int,\n",
      " |      Specify the strides of the convolution.\n",
      " |  padding : int or a tuple/list of 2 int,\n",
      " |      If padding is non-zero, then the input is implicitly zero-padded\n",
      " |      on both sides for padding number of points\n",
      " |  dilation : int or tuple/list of 2 int\n",
      " |      Specifies the dilation rate to use for dilated convolution.\n",
      " |  groups : int\n",
      " |      Controls the connections between inputs and outputs.\n",
      " |      At groups=1, all inputs are convolved to all outputs.\n",
      " |      At groups=2, the operation becomes equivalent to having two conv\n",
      " |      layers side by side, each seeing half the input channels, and producing\n",
      " |      half the output channels, and both subsequently concatenated.\n",
      " |  layout : str, default 'NCHW'\n",
      " |      Dimension ordering of data and weight. Can be 'NCHW', 'NHWC', etc.\n",
      " |      'N', 'C', 'H', 'W' stands for batch, channel, height, and width\n",
      " |      dimensions respectively. Convolution is applied on the 'H' and\n",
      " |      'W' dimensions.\n",
      " |  in_channels : int, default 0\n",
      " |      The number of input channels to this layer. If not specified,\n",
      " |      initialization will be deferred to the first time `forward` is called\n",
      " |      and `in_channels` will be inferred from the shape of input data.\n",
      " |  activation : str\n",
      " |      Activation function to use. See :func:`~mxnet.ndarray.Activation`.\n",
      " |      If you don't specify anything, no activation is applied\n",
      " |      (ie. \"linear\" activation: `a(x) = x`).\n",
      " |  use_bias : bool\n",
      " |      Whether the layer uses a bias vector.\n",
      " |  weight_initializer : str or `Initializer`\n",
      " |      Initializer for the `weight` weights matrix.\n",
      " |  bias_initializer : str or `Initializer`\n",
      " |      Initializer for the bias vector.\n",
      " |  \n",
      " |  \n",
      " |  Inputs:\n",
      " |      - **data**: 4D input tensor with shape\n",
      " |        `(batch_size, in_channels, height, width)` when `layout` is `NCW`.\n",
      " |        For other layouts shape is permuted accordingly.\n",
      " |  \n",
      " |  Outputs:\n",
      " |      - **out**: 4D output tensor with shape\n",
      " |        `(batch_size, channels, out_height, out_width)` when `layout` is `NCW`.\n",
      " |        out_height and out_width are calculated as::\n",
      " |  \n",
      " |            out_height = floor((height+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0])+1\n",
      " |            out_width = floor((width+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1])+1\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Conv2D\n",
      " |      _Conv\n",
      " |      mxnet.gluon.block.HybridBlock\n",
      " |      mxnet.gluon.block.Block\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, channels, kernel_size, strides=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, layout='NCHW', activation=None, use_bias=True, weight_initializer=None, bias_initializer='zeros', in_channels=0, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _Conv:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  hybrid_forward(self, F, x, weight, bias=None)\n",
      " |      Overrides to construct symbolic graph for this `Block`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : Symbol or NDArray\n",
      " |          The first input tensor.\n",
      " |      *args : list of Symbol or list of NDArray\n",
      " |          Additional input tensors.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from mxnet.gluon.block.HybridBlock:\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Registers parameters.\n",
      " |  \n",
      " |  export(self, path)\n",
      " |      Export HybridBlock to json format that can be loaded by `mxnet.mod.Module`\n",
      " |      or the C++ interface.\n",
      " |      \n",
      " |      .. note:: When there are only one input, it will have name `data`. When there\n",
      " |                Are more than one inputs, they will be named as `data0`, `data1`, etc.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          Path to save model. Two files `path-symbol.json` and `path-0000.params`\n",
      " |          will be created.\n",
      " |  \n",
      " |  forward(self, x, *args)\n",
      " |      Defines the forward computation. Arguments can be either\n",
      " |      :py:class:`NDArray` or :py:class:`Symbol`.\n",
      " |  \n",
      " |  hybridize(self, active=True)\n",
      " |      Activates or deactivates :py:class:`HybridBlock` s recursively. Has no effect on\n",
      " |      non-hybrid children.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      active : bool, default True\n",
      " |          Whether to turn hybrid on or off.\n",
      " |  \n",
      " |  infer_shape(self, *args)\n",
      " |      Infers shape of Parameters from inputs.\n",
      " |  \n",
      " |  register_child(self, block)\n",
      " |      Registers block as a child of self. :py:class:`Block` s assigned to self as\n",
      " |      attributes will be registered automatically.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from mxnet.gluon.block.Block:\n",
      " |  \n",
      " |  __call__(self, *args)\n",
      " |      Calls forward. Only accepts positional arguments.\n",
      " |  \n",
      " |  collect_params(self)\n",
      " |      Returns a :py:class:`ParameterDict` containing this :py:class:`Block` and all of its\n",
      " |      children's Parameters.\n",
      " |  \n",
      " |  initialize(self, init=<mxnet.initializer.Uniform object at 0x116ccb668>, ctx=None, verbose=False)\n",
      " |      Initializes :py:class:`Parameter` s of this :py:class:`Block` and its children.\n",
      " |      \n",
      " |      Equivalent to ``block.collect_params().initialize(...)``\n",
      " |  \n",
      " |  load_params(self, filename, ctx, allow_missing=False, ignore_extra=False)\n",
      " |      Load parameters from file.\n",
      " |      \n",
      " |      filename : str\n",
      " |          Path to parameter file.\n",
      " |      ctx : Context or list of Context\n",
      " |          Context(s) initialize loaded parameters on.\n",
      " |      allow_missing : bool, default False\n",
      " |          Whether to silently skip loading parameters not represents in the file.\n",
      " |      ignore_extra : bool, default False\n",
      " |          Whether to silently ignore parameters from the file that are not\n",
      " |          present in this Block.\n",
      " |  \n",
      " |  name_scope(self)\n",
      " |      Returns a name space object managing a child :py:class:`Block` and parameter\n",
      " |      names. Should be used within a ``with`` statement::\n",
      " |      \n",
      " |          with self.name_scope():\n",
      " |              self.dense = nn.Dense(20)\n",
      " |  \n",
      " |  save_params(self, filename)\n",
      " |      Save parameters to file.\n",
      " |      \n",
      " |      filename : str\n",
      " |          Path to file.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from mxnet.gluon.block.Block:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  name\n",
      " |      Name of this :py:class:`Block`, without '_' in the end.\n",
      " |  \n",
      " |  params\n",
      " |      Returns this :py:class:`Block`'s parameter dictionary (does not include its\n",
      " |      children's parameters).\n",
      " |  \n",
      " |  prefix\n",
      " |      Prefix of this :py:class:`Block`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gluon.nn.Conv2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = nd.array([20, 30, 40])\n",
    "a.attach_grad()\n",
    "\n",
    "b = nd.array([20, 30, 40])\n",
    "b.attach_grad()\n",
    "\n",
    "with autograd.record():\n",
    "    c = F.softmax(a * b)\n",
    "    d = nd.argmax(c)\n",
    "    loss = d.sum()\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1 1]\n",
      "<NDArray 2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "a = nd.array([[0.2, 0.6, 0.2], [0.4, 0.5, 0.1]])\n",
    "b = nd.sample_multinomial(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SoftmaxCrossEntropyLoss in module mxnet.gluon.loss:\n",
      "\n",
      "class SoftmaxCrossEntropyLoss(Loss)\n",
      " |  Computes the softmax cross entropy loss. (alias: SoftmaxCELoss)\n",
      " |  \n",
      " |  If `sparse_label` is `True` (default), label should contain integer\n",
      " |  category indicators:\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |      \\DeclareMathOperator{softmax}{softmax}\n",
      " |  \n",
      " |      p = \\softmax({pred})\n",
      " |  \n",
      " |      L = -\\sum_i \\log p_{i,{label}_i}\n",
      " |  \n",
      " |  `label`'s shape should be `pred`'s shape with the `axis` dimension removed.\n",
      " |  i.e. for `pred` with shape (1,2,3,4) and `axis = 2`, `label`'s shape should\n",
      " |  be (1,2,4).\n",
      " |  \n",
      " |  If `sparse_label` is `False`, `label` should contain probability distribution\n",
      " |  and `label`'s shape should be the same with `pred`:\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |      p = \\softmax({pred})\n",
      " |  \n",
      " |      L = -\\sum_i \\sum_j {label}_j \\log p_{ij}\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  axis : int, default -1\n",
      " |      The axis to sum over when computing softmax and entropy.\n",
      " |  sparse_label : bool, default True\n",
      " |      Whether label is an integer array instead of probability distribution.\n",
      " |  from_logits : bool, default False\n",
      " |      Whether input is a log probability (usually from log_softmax) instead\n",
      " |      of unnormalized numbers.\n",
      " |  weight : float or None\n",
      " |      Global scalar weight for loss.\n",
      " |  batch_axis : int, default 0\n",
      " |      The axis that represents mini-batch.\n",
      " |  \n",
      " |  \n",
      " |  Inputs:\n",
      " |      - **pred**: the prediction tensor, where the `batch_axis` dimension\n",
      " |        ranges over batch size and `axis` dimension ranges over the number\n",
      " |        of classes.\n",
      " |      - **label**: the truth tensor. When `sparse_label` is True, `label`'s\n",
      " |        shape should be `pred`'s shape with the `axis` dimension removed.\n",
      " |        i.e. for `pred` with shape (1,2,3,4) and `axis = 2`, `label`'s shape\n",
      " |        should be (1,2,4) and values should be integers between 0 and 2. If\n",
      " |        `sparse_label` is False, `label`'s shape must be the same as `pred`\n",
      " |        and values should be floats in the range `[0, 1]`.\n",
      " |      - **sample_weight**: element-wise weighting tensor. Must be broadcastable\n",
      " |        to the same shape as label. For example, if label has shape (64, 10)\n",
      " |        and you want to weigh each sample in the batch separately,\n",
      " |        sample_weight should have shape (64, 1).\n",
      " |  \n",
      " |  Outputs:\n",
      " |      - **loss**: loss tensor with shape (batch_size,). Dimenions other than\n",
      " |        batch_axis are averaged out.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SoftmaxCrossEntropyLoss\n",
      " |      Loss\n",
      " |      mxnet.gluon.block.HybridBlock\n",
      " |      mxnet.gluon.block.Block\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, axis=-1, sparse_label=True, from_logits=False, weight=None, batch_axis=0, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  hybrid_forward(self, F, pred, label, sample_weight=None)\n",
      " |      Overrides to construct symbolic graph for this `Block`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : Symbol or NDArray\n",
      " |          The first input tensor.\n",
      " |      *args : list of Symbol or list of NDArray\n",
      " |          Additional input tensors.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Loss:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from mxnet.gluon.block.HybridBlock:\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Registers parameters.\n",
      " |  \n",
      " |  export(self, path)\n",
      " |      Export HybridBlock to json format that can be loaded by `mxnet.mod.Module`\n",
      " |      or the C++ interface.\n",
      " |      \n",
      " |      .. note:: When there are only one input, it will have name `data`. When there\n",
      " |                Are more than one inputs, they will be named as `data0`, `data1`, etc.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          Path to save model. Two files `path-symbol.json` and `path-0000.params`\n",
      " |          will be created.\n",
      " |  \n",
      " |  forward(self, x, *args)\n",
      " |      Defines the forward computation. Arguments can be either\n",
      " |      :py:class:`NDArray` or :py:class:`Symbol`.\n",
      " |  \n",
      " |  hybridize(self, active=True)\n",
      " |      Activates or deactivates :py:class:`HybridBlock` s recursively. Has no effect on\n",
      " |      non-hybrid children.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      active : bool, default True\n",
      " |          Whether to turn hybrid on or off.\n",
      " |  \n",
      " |  infer_shape(self, *args)\n",
      " |      Infers shape of Parameters from inputs.\n",
      " |  \n",
      " |  register_child(self, block)\n",
      " |      Registers block as a child of self. :py:class:`Block` s assigned to self as\n",
      " |      attributes will be registered automatically.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from mxnet.gluon.block.Block:\n",
      " |  \n",
      " |  __call__(self, *args)\n",
      " |      Calls forward. Only accepts positional arguments.\n",
      " |  \n",
      " |  collect_params(self)\n",
      " |      Returns a :py:class:`ParameterDict` containing this :py:class:`Block` and all of its\n",
      " |      children's Parameters.\n",
      " |  \n",
      " |  initialize(self, init=<mxnet.initializer.Uniform object at 0x10f963668>, ctx=None, verbose=False)\n",
      " |      Initializes :py:class:`Parameter` s of this :py:class:`Block` and its children.\n",
      " |      \n",
      " |      Equivalent to ``block.collect_params().initialize(...)``\n",
      " |  \n",
      " |  load_params(self, filename, ctx, allow_missing=False, ignore_extra=False)\n",
      " |      Load parameters from file.\n",
      " |      \n",
      " |      filename : str\n",
      " |          Path to parameter file.\n",
      " |      ctx : Context or list of Context\n",
      " |          Context(s) initialize loaded parameters on.\n",
      " |      allow_missing : bool, default False\n",
      " |          Whether to silently skip loading parameters not represents in the file.\n",
      " |      ignore_extra : bool, default False\n",
      " |          Whether to silently ignore parameters from the file that are not\n",
      " |          present in this Block.\n",
      " |  \n",
      " |  name_scope(self)\n",
      " |      Returns a name space object managing a child :py:class:`Block` and parameter\n",
      " |      names. Should be used within a ``with`` statement::\n",
      " |      \n",
      " |          with self.name_scope():\n",
      " |              self.dense = nn.Dense(20)\n",
      " |  \n",
      " |  save_params(self, filename)\n",
      " |      Save parameters to file.\n",
      " |      \n",
      " |      filename : str\n",
      " |          Path to file.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from mxnet.gluon.block.Block:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  name\n",
      " |      Name of this :py:class:`Block`, without '_' in the end.\n",
      " |  \n",
      " |  params\n",
      " |      Returns this :py:class:`Block`'s parameter dictionary (does not include its\n",
      " |      children's parameters).\n",
      " |  \n",
      " |  prefix\n",
      " |      Prefix of this :py:class:`Block`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gluon.loss.SoftmaxCrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ -7.04850303e-03  -8.00704861e+00  -5.00704861e+00]\n",
      "<NDArray 3 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "a = nd.array([10, 2, 5])\n",
    "b = F.log_softmax(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[-8.00704861]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "action = nd.array([1])\n",
    "\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=True, from_logits=False)\n",
    "\n",
    "print(-loss(a, action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(logits):\n",
    "    a0 = logits - nd.max(logits, axis=-1, keepdims=True)\n",
    "    ea0 = nd.exp(a0)\n",
    "    z0 = nd.sum(ea0, axis=-1, keepdims=True)\n",
    "    p0 = ea0 / z0\n",
    "    return nd.sum(p0 * (nd.log(z0) - a0), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = nd.array([0.2, 0.4, 0.4])\n",
    "b = entropy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 1.09437883]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daidao/anaconda3/envs/rl/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def entropy(logits):\n",
    "    a0 = logits - tf.reduce_max(logits, axis=-1, keep_dims=True)\n",
    "    ea0 = tf.exp(a0)\n",
    "    z0 = tf.reduce_sum(ea0, axis=-1, keep_dims=True)\n",
    "    p0 = ea0 / z0\n",
    "    return tf.reduce_sum(p0 * (tf.log(z0) - a0), axis=-1)\n",
    "\n",
    "a = tf.Variable([0.2, 0.4, 0.4])\n",
    "b = entropy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.09438\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    c = sess.run(b)\n",
    "    \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daidao/anaconda3/envs/rl/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def sample(logits):\n",
    "    u = tf.random_uniform(tf.shape(logits))\n",
    "    return tf.argmax(logits - tf.log(-tf.log(u)), axis=-1)\n",
    "\n",
    "a = tf.Variable([9, 10., 2.])\n",
    "b = [sample(a) for _ in range(100)]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    c = sess.run(b)\n",
    "    \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "from mxnet import nd\n",
    "\n",
    "def sample(logits):\n",
    "    u = nd.random.uniform(shape=logits.shape)\n",
    "    return nd.argmax(logits - nd.log(-nd.log(u)), axis=-1)\n",
    "\n",
    "a = nd.array([9, 10., 2.])\n",
    "b = [int(sample(a).asscalar()) for _ in range(100)]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
